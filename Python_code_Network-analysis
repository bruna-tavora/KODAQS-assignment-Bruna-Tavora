###Network Analysis

##This script implements a keyword co-occurrence network analysis pipeline using Python. 
The objective is to transform a structured textual dataset (comma-separated keywords) into a weighted undirected 
graph that represents term associations across documents.

##Data Input> The script reads a CSV file using pandas, assuming the presence of a keywords column containing 
comma-separated terms for each observation. Each row is treated as an independent document (or unit of analysis).


####The preprocessing stage includes> Converting all keywords to lowercase; Trimming leading and trailing whitespace; 
Normalizing internal spacing using regular expressions; Removing duplicate keywords within the same document.

###Co-occurrence Extraction> For each document, the script generates all unordered keyword pairs using: itertools.combinations(sorted(set(tags)), 2)
Each pair represents a co-occurrence relationship. The collections.Counter object aggregates these pairs across
the dataset, producing frequency counts that later define edge weights.

### Graph Construction> A weighted undirected graph is created using NetworkX. Nodes: Unique keywords.
Edges: Keyword pairs that co-occur at least once. Edge weight: Frequency of co-occurrence across documents.

### Visualization Strategy: The graph is visualized using matplotlib with a force-directed layout (spring_layout). 
Key visualization parameters include > Node size scaling based on node degree. Edge width scaling proportional to edge 
weight.
****************************************************************************
# Install required libraries
!pip install networkx matplotlib --quiet

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from itertools import combinations
from collections import Counter
import re

# Upload your CSV to Colab or load it directly
df = pd.read_csv('key.csv')

# Example DataFrame (if needed)
# df = pd.DataFrame(data)

### First processing step

# Function to extract co-occurrence pairs
pair_counter = Counter()
for keywords in df['keywords']:
    tags = [kw.strip().lower() for kw in keywords.split(',')]
    pairs = combinations(sorted(set(tags)), 2)
    pair_counter.update(pairs)

# Create graph
G = nx.Graph()
for (tag1, tag2), weight in pair_counter.items():
    G.add_edge(tag1, tag2, weight=weight)

# Draw graph
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, k=0.5)
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]

nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue')
nx.draw_networkx_edges(G, pos, width=weights, alpha=0.6)
nx.draw_networkx_labels(G, pos, font_size=10)

plt.title("Co-occurrence Network of Keywords")
plt.axis('off')
plt.show()


# Highlight: Marco Temporal
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.5, seed=42)
weights = [G[u][v]['weight'] for u, v in G.edges()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)
nx.draw_networkx_labels(G, pos, font_size=9)

plt.title("Keyword Co-occurrence Network — Highlight: Marco Temporal")
plt.axis('off')
plt.show()


# Highlight: Sustainability
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.5, seed=42)
weights = [G[u][v]['weight'] for u, v in G.edges()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)
nx.draw_networkx_labels(G, pos, font_size=9)

plt.title("Keyword Co-occurrence Network — Highlight: Sustainability")
plt.axis('off')
plt.show()


# Define node size and color, highlighting "sustainability"
node_sizes = []
node_colors = []
for node in G.nodes():
    if node == "sustainability":
        node_sizes.append(1200)       # larger highlight
        node_colors.append('tomato')  # red highlight
    else:
        node_sizes.append(500)
        node_colors.append('skyblue')

# Plot graph with highlight
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.5, seed=42)
weights = [G[u][v]['weight'] for u, v in G.edges()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)
nx.draw_networkx_labels(G, pos, font_size=9)

plt.title("Keyword Co-occurrence Network — Highlight: Sustainability")
plt.axis('off')
plt.show()


# Count word pairs (co-occurrence)
pair_counter = Counter()
for keywords in df['keywords']:
    tags = [kw.strip().lower() for kw in keywords.split(',') if kw.strip()]
    pairs = combinations(sorted(set(tags)), 2)
    pair_counter.update(pairs)

# Create graph (show everything with at least 1 occurrence)
G = nx.Graph()
for (tag1, tag2), weight in pair_counter.items():
    if weight >= 1:
        G.add_edge(tag1, tag2, weight=weight)


# Visualization
plt.figure(figsize=(16, 12))
pos = nx.spring_layout(G, k=0.6, seed=42)
weights = [G[u][v]['weight'] for u, v in G.edges()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)
nx.draw_networkx_edges(G, pos, width=weights, alpha=0.4)
nx.draw_networkx_labels(G, pos, font_size=9)

plt.title("Keyword Co-occurrence Network — Expanded Network", fontsize=14)
plt.axis('off')
plt.show()


# Clean and process data
pair_counter = Counter()
for keywords in df['keywords']:
    tags = [re.sub(r'\s+', ' ', kw.strip().lower()) for kw in str(keywords).split(',')]
    pairs = combinations(sorted(set(tags)), 2)
    pair_counter.update(pairs)

# Select the 40 most frequent pairs
top_pairs = pair_counter.most_common(40)

# Create graph
G = nx.Graph()
for (tag1, tag2), weight in top_pairs:
    G.add_edge(tag1, tag2, weight=weight)

# Visualize graph
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.5, seed=42)
weights = [G[u][v]['weight'] for u, v in G.edges()]
node_sizes = [700 + 100 * G.degree(n) for n in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.9)
nx.draw_networkx_edges(G, pos, width=weights, alpha=0.6)
nx.draw_networkx_labels(G, pos, font_size=10)

plt.title("Keyword Co-occurrence Network — Top 40 Pairs")
plt.axis('off')
plt.show()


# Cleaner layout visualization
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.7, seed=42)

weights = [G[u][v]['weight'] for u, v in G.edges()]
scaled_weights = [w * 0.8 for w in weights]
node_sizes = [500 + 100 * G.degree(n) for n in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.9)
nx.draw_networkx_edges(G, pos, width=scaled_weights, edge_color='gray', alpha=0.4)
nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')

plt.title("Keyword Co-occurrence Network — Top 40 Pairs", fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.show()


# Select the 50 most frequent pairs
top_pairs = pair_counter.most_common(50)

# Build graph
G = nx.Graph()
for (tag1, tag2), weight in top_pairs:
    G.add_edge(tag1, tag2, weight=weight)

# Clean visualization
plt.figure(figsize=(14, 10))
pos = nx.spring_layout(G, k=0.7, seed=42)

weights = [G[u][v]['weight'] for u, v in G.edges()]
scaled_weights = [w * 0.6 for w in weights]
node_sizes = [400 + 80 * G.degree(n) for n in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.9)
nx.draw_networkx_edges(G, pos, width=scaled_weights, edge_color='gray', alpha=0.4)
nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')

plt.title("Keyword Co-occurrence Network — Top 50 Pairs", fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.show()
